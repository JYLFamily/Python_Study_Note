{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 数据集划分方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### k 折交叉验证及其变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index:  [2 3 4 5] test_index:  [0 1]\ntrain_index:  [0 1 4 5] test_index:  [2 3]\ntrain_index:  [0 1 2 3] test_index:  [4 5]\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "target = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    print(\"train_index: \", train_index, \"test_index: \", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index:  [1 2 4 5] test_index:  [0 3]\ntrain_index:  [0 2 3 5] test_index:  [1 4]\ntrain_index:  [0 1 3 4] test_index:  [2 5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "target = np.array([1, 1, 1, 2, 2, 2])\n",
    "# groups = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "sfk = StratifiedKFold(n_splits=3)\n",
    "\n",
    "for train_index, test_index in sfk.split(data, target):\n",
    "    print(\"train_index: \", train_index, \"test_index: \", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1\n0  1   2\n1  3   4\n2  7   8\n3  9  10\n   0\n0  1\n1  2\n2  4\n3  5\n    0   1\n0   5   6\n1  11  12\n   0\n0  3\n1  6\n------------------------------------------------\n    0   1\n0   1   2\n1   5   6\n2   7   8\n3  11  12\n   0\n0  1\n1  3\n2  4\n3  6\n   0   1\n0  3   4\n1  9  10\n   0\n0  2\n1  5\n------------------------------------------------\n    0   1\n0   3   4\n1   5   6\n2   9  10\n3  11  12\n   0\n0  2\n1  3\n2  5\n3  6\n   0  1\n0  1  2\n1  7  8\n   0\n0  1\n1  4\n------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "target = np.array([1, 2, 3, 4, 5, 6])\n",
    "groups = np.array([0, 1, 2, 4, 5, 6])\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "group_kfold\n",
    "\n",
    "for train_index, test_index in group_kfold.split(data, target, groups):\n",
    "    print(pd.DataFrame(data[train_index, :]))\n",
    "    print(pd.DataFrame(target[train_index]))\n",
    "    print(pd.DataFrame(data[test_index, :]))\n",
    "    print(pd.DataFrame(target[test_index]))\n",
    "    print(\"------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 留1/P法及其变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index:  [1 2 3 4 5] test_index:  [0]\ntrain_index:  [0 2 3 4 5] test_index:  [1]\ntrain_index:  [0 1 3 4 5] test_index:  [2]\ntrain_index:  [0 1 2 4 5] test_index:  [3]\ntrain_index:  [0 1 2 3 5] test_index:  [4]\ntrain_index:  [0 1 2 3 4] test_index:  [5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "target = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(data)\n",
    "\n",
    "for train_index, test_index in loo.split(data):\n",
    "    print(\"train_index: \", train_index, \"test_index: \", test_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机划分及其变体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_index:  [0 1 4 3] test_index:  [2 5]\ntrain_index:  [2 0 5 1] test_index:  [4 3]\ntrain_index:  [5 0 3 1] test_index:  [2 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]])\n",
    "target = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "ss = ShuffleSplit(n_splits=3\n",
    "                  , test_size=1/3\n",
    "                  , train_size=2/3)\n",
    "\n",
    "ss.get_n_splits()\n",
    "\n",
    "for train_index, test_index in ss.split(data):\n",
    "    print(\"train_index: \", train_index, \"test_index: \", test_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 超参数优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81144781144781142"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.stats import randint as hp_randint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def report(results, n_top=3):\n",
    "    results = pd.DataFrame(clf.cv_results_).sort_values(by = ['rank_test_score']).head(n_top)\n",
    "    results = results.loc[:, ['rank_test_score', 'std_test_score', 'mean_test_score', 'params']].reset_index()\n",
    "    print(results.T)\n",
    "\n",
    "\n",
    "\n",
    "data, target = load_digits(return_X_y=True)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "pararmeters = {\n",
    "    'criterion':[\"gini\", \"entropy\"],\n",
    "    'max_features':hp_randint(1, 10), # 1~10 均匀分布 \n",
    "    'max_depth':hp_randint(1, 4)  # 1~4 均匀分布 \n",
    "}\n",
    "\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, \n",
    "                                                                    target, \n",
    "                                                                    test_size=0.33, \n",
    "                                                                    random_state=42)\n",
    "\n",
    "clf = RandomizedSearchCV(rf,\n",
    "                         pararmeters,\n",
    "                         n_iter=10,\n",
    "                         n_jobs=4)\n",
    "\n",
    "# Call predict on the estimator with the best found parameters.\n",
    "clf.fit(data_train, target_train)\n",
    "clf.predict(data_test)\n",
    "\n",
    "accuracy_score(target_test, clf.predict(data_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过交叉验证计算得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "[joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ccbd96b7e13c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# 默认是 3 折 StratifiedKFold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\puhui\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    138\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                                               fit_params)\n\u001b[1;32m--> 140\u001b[1;33m                       for train, test in cv_iter)\n\u001b[0m\u001b[0;32m    141\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\puhui\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aborting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m             \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m             \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_effective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\puhui\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_initialize_backend\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m             return self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n\u001b[1;32m--> 540\u001b[1;33m                                            **self._backend_args)\n\u001b[0m\u001b[0;32m    541\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFallbackToBackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[1;31m# Recursively initialize the backend in case of requested fallback.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\puhui\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mconfigure\u001b[1;34m(self, n_jobs, parallel, **backend_args)\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malready_forked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             raise ImportError(\n\u001b[1;32m--> 299\u001b[1;33m                 \u001b[1;34m'[joblib] Attempting to do parallel computing '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;34m'without protecting your import on a system that does '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[1;34m'not support forking. To use parallel-computing in a '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: [joblib] Attempting to do parallel computing without protecting your import on a system that does not support forking. To use parallel-computing in a script, you must protect your main loop using \"if __name__ == '__main__'\". Please see the joblib documentation on Parallel for more information"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data, target = load_digits(return_X_y=True)\n",
    "\n",
    "# np.logspace(-10, -1, 10) 10^-10, -10^-9, -10^-8, -10^-7, -10^-6, -10^-5, -10^-4, -10^-3, -10^-2, -10^-1\n",
    "for i, c in enumerate(np.logspace(-10, -1, 10), start=1):\n",
    "    clf = SVC(C=c, kernel='linear')\n",
    "    # 默认是 3 折 StratifiedKFold \n",
    "    print(i, cross_val_score(clf, data, target, n_jobs=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对每个输入数据点产生交叉验证估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [3 3 3 ..., 1 3 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [3 3 3 ..., 1 3 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 [3 3 3 ..., 1 3 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [3 3 3 ..., 1 3 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [3 3 3 ..., 1 3 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 [0 1 1 ..., 1 0 3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 [0 1 1 ..., 8 9 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [0 1 1 ..., 8 9 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 [0 1 1 ..., 8 9 8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 [0 1 1 ..., 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "data, target = load_digits(return_X_y=True)\n",
    "\n",
    "for i, c in enumerate(np.logspace(-10, -1, 10), start=1):\n",
    "    clf = SVC(C=c, kernel='linear')\n",
    "    # 类似前面 cross_val_score() 通过交叉验证得到 estimator（所选择模型 + 参数）在数据集 X，y 上通过 cv 得到的 scoring 指标，这里不是得到指标而是输出预测的 y_label。\n",
    "    # 用 3 折交叉验证举例，\n",
    "    # 用 1、2 两部分数据 train 预测 3 部分数据、\n",
    "    # 用 1、3 两部分数据 train 预测 2 部分数据、\n",
    "    # 用 2、3 两部分数据 train 预测 1 部分数据，最终将三部分预测结果拼接起来返回\n",
    "    print(i, cross_val_predict(clf, data, target, n_jobs=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
